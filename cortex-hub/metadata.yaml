version: hub/v1alpha
kind: installable
name: SciBERT
image: allenai-logo.png
summary: |
  SciBERT is a BERT model trained on scientific text.
description: |
  SciBERT is a BERT model trained on scientific text. SciBERT is trained on papers from the corpus of [semanticscholar.org](https://semanticscholar.org). Corpus size is 1.14M papers, 3.1B tokens. We use the full text of the papers in training, not just abstracts.
  SciBERT has its own vocabulary (scivocab) that's built to best match the training corpus. We trained cased and uncased versions. We also include models trained on the original BERT vocabulary (basevocab) for comparison.
  It results in state-of-the-art performance on a wide range of scientific domain nlp tasks. 

  The details of the evaluation are in the paper. Evaluation code and data are included in this repo.

  A default NER model is trained based on the dataset provided using AllenNLP following the instructions provided in  "Train a new model using AllenNLP" section.
license: ../LICENSE.txt
contributor:
  name: Contributor
  email: no-reply@email.com
  url: http://www.cognitivescale.com
assets:
  - type: model
    name: NER Model
    location: ../models/ner/model.tar.gz
  - type: certifai-scan
    name: Certifai Scan
    location: certifai/certifai.yaml
